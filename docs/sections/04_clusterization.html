<h2 id="clusterization">Generating Clusters and Labels</h2>
<blockquote>Since the embedding vectors capture semantic meaning, could we leverage them to generate our own,
    potentially improved, classification of the questions?</blockquote>
<p>The semantic relationships between questions are encoded within their embedding vectors. Therefore, we can use these
    vectors as input for standard clustering algorithms to group the questions based on their content, effectively
    deriving a new classification automatically.</p>
<p>The original questions were assigned two hierarchical tags. In this exercise, we focus on automatically recreating
    the primary (first-level) tag category using a hierarchical clustering algorithm. Hierarchical clustering
    builds a tree-like hierarchy of nested clusters by progressively merging similar data points or
    splitting dissimilar ones, without requiring a predefined number of clusters.</p>
<p>We configured the algorithm's parameters by specifying a distance threshold
    to yield a final count of clusters similar to the number of original first-level tag categories (which was 5).</p>
<p>This process resulted in a classification with 6 distinct clusters. The plot below visualizes the embeddings colored
    by their generated cluster membership after dimensionality reduction (e.g., using t-SNE or PCA):</p>
<p>The result is a 6 cluster classification and the reduced plot follows (below, we show again the previous
    classification):
    <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
        src="./html/clusterization_hierarchical.html" height="500px" width="100%"></iframe>
<figcaption>The clusterer classification</figcaption>
<iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
    src="./html/embedding_tsne_2.html" height="500px" width="100%"></iframe>
<figcaption>The original classification</figcaption>
</p>
<p>Initially, these generated clusters are simply numbered sequentially by the algorithm (e.g., 0, 1, 2...). To make
    this automated classification interpretable and useful, we need to assign meaningful labels to these clusters
    programmatically.</p>
<p>AI, specifically Large Language Models (LLMs), can assist with this labeling task. We used the Gemini model
    '<code>models/gemini-1.5-flash</code>'. For each generated cluster, we constructed a prompt containing all the
    question texts belonging to that single cluster and asked the model to generate a concise, descriptive tag
    summarizing the common theme.</p>

<pre><code class="language-python">
    # Adjust prompt for tag generation
    prompt = (f"Generate a single tag of length between 1 and {n_tags} words that best describes the main topic of these questions: {all_text}. "
                f"Answer with only the tag, in Italian language, where words are all lowercase and separated by an underscore.")
    if tags_to_avoid:
        prompt += f"Avoid words coming from these tags (split on the underscore if necessary): \"{tags_to_avoid}\""
</code></pre>

<p>An additional mechanism was included in the process to prevent the LLM from reusing tags already generated for
    previous clusters, as this repetition can sometimes occur. And these are the results:</p>

{% include 'html/llm_tags.html' %}

<p>The questions are reassigned as follows:
    <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
        src="./html/tag_dispersion.html" height="500px" width="100%"></iframe>
</p>

<blockquote>A traditional clusterization technique, applied to the embedding vectors, made us find a fresh
    classification for our questions.
    But how can we know whether this is better than the original one?
</blockquote>

<p>Comparing the new 6-cluster, LLM-labeled classification to the original 5 first-level tags to demonstrate improvement
    requires looking at it from a few angles.
    There's rarely a single "best" metric, and probably the best comparison method is a deeper visual analysis of the
    plots.
    However, some quantitative metrics exist for evaluating how well-separated and compact the clusters:
<ul>
    <li>Silhouette Score: Measures how similar each point is to its own cluster compared to other clusters. Ranges from
        -1 to 1. Higher values (closer to 1) indicate dense, well-separated clusters. Calculate the average Silhouette
        score for both classifications.</li>
    <li>Davies-Bouldin Index: Measures the average similarity between each cluster and its most similar one (based on
        distance between centroids and cluster spread). Lower values indicate better separation.</li>
    <li>Calinski-Harabasz Index (Variance Ratio Criterion): Measures the ratio of between-cluster dispersion to
        within-cluster dispersion. Higher values indicate better-defined (denser and more separated) clusters.</li>
</ul>
The package <code>scikit-learn.metrics</code> helps us in calculating these values.
</p>

<code>
    {% include 'html/cluster_indexes.html' %}    
</code>

<p>According to these quantitative metrics, the clustering derived from the embeddings appears to fit the data structure
    better than the original tag classification. However, achieving the 'best' score might not be the primary focus in
    this context. The source questions themselves often contain ambiguities and overlap categories — perhaps
    intentionally, designed to test nuanced understanding for the driving exam — making any single categorization
    inherently challenging.</p>

<p>What is more significant, therefore, is the demonstration of a methodology that integrates machine learning
    techniques (embeddings, clustering), generative AI (for automated labeling), and quantitative analysis to tackle the
    problem of text classification. </p>

<blockquote>
    This integrated approach enables us first to discover or refine a classification
    system that reflects the dataset's underlying structure, and then to build AI agents capable of automatically
    assigning new texts to relevant categories, regardless of whether those categories were predefined or discovered
    automatically through methods like clustering.
</blockquote>