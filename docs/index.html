<!doctype html>
<html class="no-js" lang="en">
<head>
      <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>opal-classifier - Opal Classifier</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=9" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Source+Serif+Pro:400,700">
    <link rel="icon" type="image/png" sizes="32x32" href="./images/favicon.ico">
    <link rel="stylesheet" type="text/css" href="./theme/css/main.css">
    <link rel="stylesheet" type="text/css" href="./theme/css/style.css">
    <script src='https://cdn.plot.ly/plotly-2.32.0.min.js'></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css"
        integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js"
        integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js"
        integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>


</head>
<body itemscope itemtype="http://schema.org/Blog">
<header class="site-header">
    <a href=".">
    </a>
    <h1 itemprop="name"><a href="."></a></h1>
</header>
    <section class="main-content">
  <article>
    <header>
      <h2>Opal Classifier</h2>
    </header>
    

    
    <div class="content-column">
        <header>
            <h1>Classifying Text</h1>
        </header>

        <article>

<blockquote>What we are talking about.</blockquote>

<div class="qa-box">
    <p><b>What is text classifying?</b><br>
        Text classification is the task of automatically assigning predefined labels or categories to pieces
        of
        text.</p>
    <p><b>Why is it important?</b><br>
        Text classification is important because it allows us to automatically make sense of and organize
        large
        volumes of text data, which would be impossible or incredibly time-consuming to do manually.</p>
    <p><b>Which techniques are commonly used?</b><br>
        Common methods range from traditional algorithms (like Naive Bayes, SVM) to deep learning (like
        LSTMs or
        Transformers/BERT).</p>
    <p><b>Why does this suit very well with ML and AI approach?</b><br>
        Because ML/AI excels at automatically learning complex patterns from text data to assign categories
        accurately, handling the nuance and scale of language far better than manual rules.</p>
    <p><b>What is the base technique behind this approach?</b><br>
        The base technique is typically supervised machine learning. This means you train an algorithm using
        a
        large dataset of text examples that have already been manually assigned the correct category labels.
        The
        algorithm learns the patterns and relationships between the text features (words, embeddings, etc.)
        and
        the corresponding labels, enabling it to predict labels for new, unseen text.</p>
    <p><b>What is the embedding?</b><br>
        Essentially, an embedding is the numerical vector (a list of numbers) used to represent a piece of
        text
        for the machine learning model. It's designed so that texts with similar meanings result in vectors
        that are mathematically close to each other.</p>
    <p><b>But how can I transform my text into an embedding?</b><br>
        You typically use a pre-trained AI model (accessed via an API or code library) to convert your text
        into
        its numerical embedding vector.
    </p>
    <p><b>How can this be profitable?</b><br>
        It's profitable by automating tasks (cutting costs) and extracting valuable insights from text data
        to
        improve customer experience, boost sales, and make smarter business decisions.
    </p>
    <p><b>What are some practical examples?</b><br>
        Email spam filtering,
        customer support ticket routing,
        sentiment analysis,
        content moderation,
        topic labelling,
        intent recognition
    </p>
</div>

<blockquote>Given that, we now want to build an example of how text classifying could work. <br>
    As our sample
    dataset, we will use the questions and answers (Q&As;) from the Italian driving license
    examination </blockquote>
            <div class="toc-box">
                <h3>Index</h3>
                <ul>
                    <span>The Dataset</span>
                    <li><a href="#data_presentation">Dataset Presentation</a></li>
                    <ul>
                        <li><a href="#data_presentation_embed_text">Embed the Text</a></li>
                        <li><a href="#data_presentation_encode_output">Encode the Output</a></li>
                        <li><a href="#data_presentation_network_setup">Setup the Network</a></li>
                    </ul>                                        
                    <li><a href="#model_results">Model Results</a></li>
                    <li><a href="#embeddings">It's All About the Embeddings</a></li>
                    <li><a href="#clusterization">Challenging the AI: a Clusterization Approach</a></li>
                </ul>
            </div>

<h2 id="data_presentation">Data Presentation</h2>
<p>The questions are organized in a two-level structure, as illustrated in the following plot:
    <iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="./html/tag_hierarchy.html" height="525" width="100%"></iframe>
</p>

<p>The dataset contains over 7,000 questions distributed among the tags as shown here:
    <iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="./html/tag_counts.html" height="525" width="100%"></iframe>
</p>

<p>Each question has two tags, one for each level. Furthermore, every question has a true or false answer.
</p>

<blockquote> The primary objective is to build a classifier capable of predicting the correct categories for
    a given
    question.
</blockquote>

<h3 id="data_presentation_embed_text">Embed the Text</h3>
Each question was transformed into a numerical vector using an embedding model from the
<mark>Google GenAI</mark> suite, specifically the '<code>models/text-embedding-004</code>' <a href="#section1">[1]</a>

<pre><code class="language-python">
                # Example call to the embedding model
                embedding = google_ai_client.models.embed_content(
                    model=embedding_model, contents=text
                )
                # embedding_result typically contains the numerical vector
            </code></pre>

<p>
    The model converts the original question text into a vector of 768 numbers. <br>
    This vector serves as the ideal input for a neural network, while the network's output should represent
    the target categories.
</p>


<h3 id="data_presentation_encode_output">Encode the Output</h3>
<p>Given \(n\) possible categories, we considered two options for representing the target output:</p>
<ul>
    <li>Using a single floating-point number representing a category index (e.g., in the range \([1, n]\)).
    </li>
    <li>Using an \(n\)-element array where each element is in the range \([0, 1]\), representing the
        probability that the corresponding category is associated with the question.</li>
</ul>
<p> We chose the second option to better handle cases where a question might belong to multiple categories
    simultaneously. <br>
    We assigned a fixed order to all category tags. Since there are 30 tags in this example, the target
    output for each question is a 30-element vector. <br>
    Maintaining this predefined order is crucial to ensure each tag corresponds to a consistent position in
    the output vector.
</p>

<h3 id="data_presentation_network_setup">Setup the Network</h3>
<p>Therefore, for a single question, the network processes:</p>
<ul>
    <li>A 768-element input vector, representing the embedded text.</li>
    <li>A 30-element output vector, representing the target category associations.</li>
</ul>
<blockquote>We do not explicitly inform the network about the hierarchical structure of the tags; it is free
    to learn associations independently.</blockquote>
<p>
    The network predicts category associations by assigning a probability to each tag. A higher probability
    suggests the tag is more suitable for that question. In the training dataset, the target
    output vectors consist only of 0s (for unassociated tags) and 1s (for associated tags).
</p>

<p>This is a graphical representation of the network:
    <iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="./html/network.html" height="525" width="100%"></iframe>
</p>
<h2>Section 3: The Result</h2>
<p>The result is usually presented in the form of a confusion matrix:
    <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless" src="./html/confusion_matrix_MulticlassModel01_FULL_SET.html" height="900px" width="100%"></iframe>
</p>
<p>In this heatmap, we analyze the model's predictions, showing the average predicted probability for each
    tag category, likely correlated with the true categories.
    It's evident that the main tag categories are typically assigned high probabilities by the model,
    whereas some secondary categories present more of a challenge, receiving lower average probabilities.
</p>
<p>Overall, the results are encouraging. The primary tag (level 1) predictions are highly accurate (as seen
    by the blue vertical lines), while the secondary tag (level 2) predictions are less distinct. However,
    even when the secondary tag prediction is incorrect, it often falls within the expected category group.
</p>
<p>Further investigation into the specific prediction errors could be insightful.</p>
<blockquote>Such a deep dive might
    reveal patterns in the model's mistakes or even suggest potential ambiguities or inaccuracies in the
    original tagging of the questions.</blockquote>
<p>These are the confusion counts:
    <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless" src="./html/error_heatmapMulticlassModel01_FULL_SET.html" height="800px" width="100%"></iframe>
</p>
<p>This plot analyzes the model's predictions by considering the top two most likely tags assigned to each
    question.</p>
<p>It clearly shows that prediction errors are relatively infrequent, and most of them are concentrated
    within the central part of the visualization. This central region corresponds to questions covering very
    similar topics, which makes some level of confusion understandable.</p>
<p>Other areas prone to errors involve questions about traffic and those concerning the vehicle itself. For
    these topics, the original tagging may be inherently ambiguous or difficult to assign definitively,
    likely contributing to the model's confusion.</p>
<h2>Section 4: The Embedding</h2>
<blockquote>While the neural network performs the classification task, the real magic often lies in the
    embeddings
    themselves.</blockquote>
<p>An embedding represents items (like words or sentences, in our case) as numerical vectors&mdash;lists of
    numbers. The key idea is that these vectors capture semantic meaning, so items with similar meanings are
    represented by vectors close together in the high-dimensional space. This numerical format allows
    machine learning models, like neural networks, to understand and process complex data such as text.</p>
<p>We wanted to gain a visual intuition for how these embeddings are structured. Since they are
    high-dimensional vectors (768 dimensions in this work), we need specific techniques to represent them
    visually in two or three dimensions. For this purpose, we explored dimensionality reduction techniques,
    initially focusing on PCA and t-SNE <a href="#section1">[2]</a>.</p>
<p>Both PCA (Principal Component Analysis) and t-SNE (t-Distributed Stochastic Neighbor Embedding) serve
    this function. They are used here to take these high-dimensional embedding vectors (which, with 768
    dimensions, are impossible to plot directly) and project them into a low-dimensional space. The underlying assumption is that embeddings for semantically similar concepts will cluster together
    within the high-dimensional space.</p>

<!--<p>These are the plots of the embeddings reduced with PCA:
    <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
        src="./html/embedding_pca_2.html" height="400px" width="100%"></iframe>
    <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
        src="./html/embedding_pca_3.html" height="700px" width="100%"></iframe>
</p>-->

<p>These are the plots of the embeddings reduced with t-SNE:
    <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless" src="./html/embedding_tsne_2.html" height="400px" width="100%"></iframe>
    <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless" src="./html/embedding_tsne_3.html" height="700px" width="100%"></iframe>
</p>
<p>By visualizing these embeddings we can more easily perceive which tag categories are related (appearing
    nearby in the plot with relative colors) and identify categories that the classification model may
    frequently confuse with
    one another (indicated by overlapping or indistinct clusters).</p>
<h2>Section 5: Generating Clusters and Labels</h2>
<blockquote>Since the embedding vectors capture semantic meaning, could we leverage them to generate our own,
    potentially improved, classification of the questions?</blockquote>
<p>The semantic relationships between questions are encoded within their embedding vectors. Therefore, we can use these
    vectors as input for standard clustering algorithms to group the questions based on their content, effectively
    deriving a new classification automatically.</p>
<p>The original questions were assigned two hierarchical tags. In this exercise, we focus on automatically recreating
    the primary (first-level) tag category using a hierarchical clustering algorithm. Hierarchical clustering, as
    defined earlier, builds a tree-like hierarchy of nested clusters by progressively merging similar data points or
    splitting dissimilar ones, without requiring a predefined number of clusters.</p>
<p>We configured the algorithm's parameters (e.g., by specifying the desired number of clusters or a distance threshold)
    to yield a final count of clusters similar to the number of original first-level tag categories (which was 5).</p>
<p>This process resulted in a classification with 6 distinct clusters. The plot below visualizes the embeddings colored
    by their generated cluster membership after dimensionality reduction (e.g., using t-SNE or PCA):</p>
<p><iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless" &lt;p>The result is a 6 cluster
        classification and the reduced plot follows:
        <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless" src="./html/clusterization_hierarchical.html" height="400px" width="100%"></iframe>
</p>
<p>Initially, these generated clusters are simply numbered sequentially by the algorithm (e.g., 0, 1, 2...). To make
    this automated classification interpretable and useful, we need to assign meaningful labels to these clusters
    programmatically.</p>
<p>AI, specifically Large Language Models (LLMs), can assist with this labeling task. We used the Gemini model
    '<code>models/gemini-1.5-flash</code>'. For each generated cluster, we constructed a prompt containing all the
    question texts belonging to that single cluster and asked the model to generate a concise, descriptive tag
    summarizing the common theme.</p>

<pre><code class="language-python">
    # Adjust prompt for tag generation
    prompt = (f"Generate a single tag of length between 1 and {n_tags} words that best describes the main topic of these questions: {all_text}. "
                f"Answer with only the tag, in Italian language, where words are all lowercase and separated by an underscore.")
    if tags_to_avoid:
        prompt += f"Avoid words coming from these tags (split on the underscore if necessary): \"{tags_to_avoid}\""
</code></pre>

<p>An additional mechanism was included in the process to prevent the LLM from reusing tags already generated for
    previous clusters, as this repetition can sometimes occur. And these are the results:</p>

<ul>
  <li>sicurezza_stradale_e_inquinamento</li>
  <li>segnaletica_stradale_e_veicoli</li>
  <li>regole_della_circolazione</li>
  <li>patente_di_guida_e_regole</li>
  <li>segnali_luminosi_e_pannelli</li>
  <li>precedenza_incroci</li>
</ul>    
<hr>

<note>[1] <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings">Embeddings
        Docs on Google Generative AI</a></note>

<hr>

<note>[2] PCA rotates your data so the maximum variance lies along the first new axis (PC1), the maximum
    remaining variance lies along the second axis (PC2, perpendicular to PC1), and so on. To visualize in
    2D, it projects your 768-dimensional points onto the first two principal components (PC1 and PC2),
    discarding the other dimensions.
    t-SNE focuses on preserving the local structure. It calculates a measure of similarity between pairs of
    points in the high-dimensional space (nearby points are considered similar). It then tries to arrange
    the points in the low-dimensional (2D) map so that similar high-dimensional points are close together
    and dissimilar points are far apart. It uses probabilities and an optimization process to achieve this
    arrangement.
</note>
        </article>

        <footer>
            <p>&copy; 2025 Opal Company</p>
        </footer>


  </article>
    </section>

<footer class="site-footer">
    <p>&copy; Carlo Caligaris 4 Opal </p>
</footer></body>
</html>