<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yet Another NN Powered Classifier</title>
    <link rel="stylesheet" href="style.css">
    <script src='https://cdn.plot.ly/plotly-2.32.0.min.js'></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css"
        integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js"
        integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js"
        integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
</head>

<body>
    <div class="content-column">
        <header>
            <h1>Neural Networks for Text Classifiyng</h1>
        </header>

        <article>
            <p>
                In this short article, we will demonstrate how to use embeddings and neural networks to classify short
                pieces of text. <br>
                As our sample dataset, we will use the questions and answers (Q&As) from the Italian driving license
                examination
            </p>

            <h2>Section 1: The Data</h2>

            <p>The questions are organized in a two-level structure, as illustrated in the following plot:
                <iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="tag_hierarchy.html"
                    height="525" width="100%"></iframe>
            </p>

            <p>The dataset contains over 7,000 questions distributed among the tags as shown here:
                <iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="tag_counts.html"
                    height="525" width="100%"></iframe>
            </p>

            <p>Each question has two tags, one for each level. Furthermore, every question has a true or false answer.
            </p>

            <h2>Section 2: Main Objective</h2>

            <p> The primary objective is to build a classifier capable of predicting the correct categories for a given
                question. <br>
                Secondly, we will investigate whether the same approach can be used to predict the question's answer
                (true/false).</p>

            <h2>Section 3: The Approach</h2>
            <h3>Section 3.1: Embed the Text</h3>
            Each question was transformed into a numerical vector using an embedding model from the
            <mark>Google GenAI</mark> suite, specifically the '<code>models/text-embedding-004</code>'

            <pre><code class="language-python">
                # Example call to the embedding model
                embedding = google_ai_client.models.embed_content(
                    model=embedding_model, contents=text
                )
                # embedding_result typically contains the numerical vector
            </code></pre>

            <p>
                The model converts the original question text into a vector of 768 numbers. <br>
                This vector serves as the ideal input for a neural network, while the network's output should represent
                the target categories.
            </p>

            <h3>Section 3.2: Encode the Output</h3>
            <p>Given \(n\) possible categories, we considered two options for representing the target output:</p>
            <ul>
                <li>Using a single floating-point number representing a category index (e.g., in the range \([1, n]\)).
                </li>
                <li>Using an \(n\)-element array where each element is in the range \([0, 1]\), representing the
                    probability that the corresponding category is associated with the question.</li>
            </ul>
            <p> We chose the second option to better handle cases where a question might belong to multiple categories
                simultaneously. <br>
                We assigned a fixed order to all category tags. Since there are 30 tags in this example, the target
                output for each question is a 30-element vector. <br>
                Maintaining this predefined order is crucial to ensure each tag corresponds to a consistent position in
                the output vector.
            </p>

            <h3>Section 3.3: Setup the Network</h3>
            <p>Therefore, for a single question, the network processes:</p>
            <ul>
                <li>A 768-element input vector, representing the embedded text.</li>
                <li>A 30-element output vector, representing the target category associations.</li>
            </ul>
            <p>Note: We do not explicitly inform the network about the hierarchical structure of the tags; it is free to
                learn associations independently.
                The network predicts category associations by assigning a probability to each tag. A higher probability
                suggests the tag is more suitable for that question. In the training dataset, the target
                output vectors consist only of 0s (for unassociated tags) and 1s (for associated tags).
            </p>

            <p>This is a graphical representation of the network:
                <iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="network.html"
                    height="525" width="100%"></iframe>
            </p>

            <h2>Section 4: The Result</h2>
            <p>The result is usually presented in the form of a confusion matrix:
                <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
                    src="confusion_matrix_MulticlassModel01_FULL_SET.html" height="800px" width="60%"></iframe>
            </p>
            <p>In this heatmap, we analyze the model's predictions, showing the average predicted probability for each
                tag category, likely correlated with the true categories.
                It's evident that the main tag categories are typically assigned high probabilities by the model,
                whereas some secondary categories present more of a challenge, receiving lower average probabilities.
            </p>
            <p>Overall, the results are encouraging. The primary tag (level 1) predictions are highly accurate (as seen
                by the blue vertical lines), while the secondary tag (level 2) predictions are less distinct. However,
                even when the secondary tag prediction is incorrect, it often falls within the expected category group.
            </p>
            <p>Further investigation into the specific prediction errors could be insightful. Such a deep dive might
                reveal patterns in the model's mistakes or even suggest potential ambiguities or inaccuracies in the
                original tagging of the questions.</p>
            <p>These are the confusion counts:
                <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
                    src="error_heatmapMulticlassModel01_FULL_SET.html" height="800px" width="60%"></iframe>
            </p>
            <p>This plot analyzes the model's predictions by considering the top two most likely tags assigned to each
                question.</p>
            <p>It clearly shows that prediction errors are relatively infrequent, and most of them are concentrated
                within the central part of the visualization. This central region corresponds to questions covering very
                similar topics, which makes some level of confusion understandable.</p>
            <p>Other areas prone to errors involve questions about traffic and those concerning the vehicle itself. For
                these topics, the original tagging may be inherently ambiguous or difficult to assign definitively,
                likely contributing to the model's confusion.</p>

            <h2>Section 5: The Embedding</h2>
            <p>While the neural network performs the classification task, the real magic often lies in the embeddings
                themselves.</p>
            <p>An embedding represents items (like words or sentences, in our case) as numerical vectors&mdash;lists of
                numbers. The key idea is that these vectors capture semantic meaning, so items with similar meanings are
                represented by vectors close together in the high-dimensional space. This numerical format allows
                machine learning models, like neural networks, to understand and process complex data such as text.</p>
            <p>We wanted to gain a visual intuition for how these embeddings are structured. Since they are
                high-dimensional vectors (768 dimensions in this work), we need specific techniques to represent them
                visually in two or three dimensions. For this purpose, we explored dimensionality reduction techniques,
                initially focusing on PCA and t-SNE <a href="#section1">[1]</a>.</p>
            <p>Both PCA (Principal Component Analysis) and t-SNE (t-Distributed Stochastic Neighbor Embedding) serve
                this function. They are used here to take these high-dimensional embedding vectors (which, with 768
                dimensions, are impossible to plot directly) and project them into a low-dimensional space (typically 2D
                or 3D). The result can then be easily visualized as a scatter plot, where each point represents an
                original text input.</p>
            <p>The underlying assumption is that embeddings for semantically similar concepts will cluster together
                within the high-dimensional space.</p>

            <p>These are the plots of the embeddings reduced with PCA:
                <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
                    src="embedding_pca_2.html" height="400px" width="100%"></iframe>
                <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
                    src="embedding_pca_3.html" height="700px" width="100%"></iframe>
            </p>
            <p>while these are the plots of the embeddings reduced with t-SNE:
                <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
                    src="embedding_tsne_2.html" height="400px" width="100%"></iframe>
                <iframe id="igraph" scrolling="no" style="border:none;display:block" seamless="seamless"
                    src="embedding_tsne_3.html" height="700px" width="100%"></iframe>
            </p>
            <p>By visualizing these embeddings we can more easily perceive which tag categories are related (appearing
                nearby in the plot with relative colors) and identify categories that the classification model may
                frequently confuse with
                one another (indicated by overlapping or indistinct clusters).</p>


            <HR>
            <note>[1] PCA rotates your data so the maximum variance lies along the first new axis (PC1), the maximum
                remaining variance lies along the second axis (PC2, perpendicular to PC1), and so on. To visualize in
                2D, it projects your 768-dimensional points onto the first two principal components (PC1 and PC2),
                discarding the other dimensions.
                t-SNE focuses on preserving the local structure. It calculates a measure of similarity between pairs of
                points in the high-dimensional space (nearby points are considered similar). It then tries to arrange
                the points in the low-dimensional (2D) map so that similar high-dimensional points are close together
                and dissimilar points are far apart. It uses probabilities and an optimization process to achieve this
                arrangement.
            </note>



        </article>

        <footer>
            <p>&copy; 2025 Opal Company</p>
        </footer>
</body>

</html>